{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "291c7c1d",
   "metadata": {
    "id": "291c7c1d"
   },
   "outputs": [],
   "source": [
    "# Student one full name\n",
    "Student1_Name = \"Sara Bisheer Fikry\"\n",
    "\n",
    "# Student two full name\n",
    "Student2_Name = \"Menna Mohamed Abdelbaset\"\n",
    "\n",
    "# team ID\n",
    "team_ID = \"19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9818bc3c",
   "metadata": {
    "id": "9818bc3c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams, FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3f0456",
   "metadata": {
    "id": "fa3f0456"
   },
   "source": [
    "# Requirement Description\n",
    "You are given pizza orders dataset. You are required to build some Language Models from the training set. You will then use these models for predicting the masked tokens in the test set. For each order in the test set, a random word was replaced with the word mask. Your task will be to predict the mask work. This requirement is divided into three parts:\n",
    "\n",
    "## Part 1: Data Preprocessing\n",
    "In this part, you will preprocess the given dataset. Once you have preprocessed the orders, you can then use them to build the language models.\n",
    "\n",
    "## Part 2: Bi-gram Language Model\n",
    "In this part, You will build a bi-gram language model from scratch. The Bi-gram language model need to have two main functions: train, predict.\n",
    "\n",
    "## Part 3: RNN Language Model\n",
    "In this part, You will build an RNN language model from scratch. The RNN language model need to have two main functions: train, predict.\n",
    "\n",
    "---\n",
    "Let's get started :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b0a2d337",
   "metadata": {
    "id": "b0a2d337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size = 10000\n",
      "test set size = 1000\n"
     ]
    }
   ],
   "source": [
    "# Read the training and test sets\n",
    "train_sentences = pd.read_csv('pizza_train.csv')['text'].to_list()\n",
    "test_sentences = pd.read_csv('masked_pizza_test.csv')['text'].to_list()\n",
    "print(f\"train set size = {len(train_sentences)}\")\n",
    "print(f\"test set size = {len(test_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a2cf4c",
   "metadata": {
    "id": "83a2cf4c"
   },
   "source": [
    "# Part 1: Data Preproccessing\n",
    "After reading the train and test sets You will do the following steps on both train and test sets:\n",
    "Loop over the sentences to do the following\n",
    "   1. convert all characters to lower (hint: lower())\n",
    "   2. tokenize the sentence (hint: use word_tokenize())\n",
    "   3. Add the start sentence \\<s> and end sentence \\</s> tokens at the beginning and end of each tokenized sentence\n",
    "   4. Add the tokens of the sentence in a predefined set to collect the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d6148c80",
   "metadata": {
    "id": "d6148c80"
   },
   "outputs": [],
   "source": [
    "# This function takes a List of sentences to preprocess them and vocabulary to extend\n",
    "# It returns a list of sentences after preprocessing and the vocabulary it found\n",
    "def preprocess(sentences, vocab):\n",
    "    tokenized_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "\n",
    "        ################################ TODO: Preprocessing  ####################################\n",
    "        # convert all characters to lower (hint: use lower())\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        # tokenize the sentence (hint: use word_tokenize())\n",
    "        sentence = word_tokenize(sentence)\n",
    "\n",
    "        # Add the start sentence <s> and end sentence </s> tokens at the beginning and end of each tokenized sentence\n",
    "\n",
    "        sentence = ['<s>'] + sentence + ['</s>']\n",
    "        # Add the tokens of the sentence in the predefined set vocab to collect the vocabulary\n",
    "\n",
    "        for token in sentence:\n",
    "            vocab.add(token)\n",
    "        # Add the sentence to the tokenized_sentences\n",
    "        tokenized_sentences.append(sentence)\n",
    "        \n",
    "\n",
    "        #########################################################################################\n",
    "\n",
    "    return tokenized_sentences, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "1d0f0f04",
   "metadata": {
    "id": "1d0f0f04"
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "preprocessed_train_sentences, vocab = preprocess(train_sentences, vocab)\n",
    "preprocessed_test_sentences, vocab = preprocess(test_sentences, vocab)\n",
    "vocab.remove(\"mask\")\n",
    "\n",
    "\n",
    "assert type(preprocessed_train_sentences) == list, \"Type of preprocessed train should be list\"\n",
    "assert type(preprocessed_test_sentences) == list, \"Type of preprocessed test should be list\"\n",
    "assert len(preprocessed_train_sentences) == 10000, \"The number of train sentences is not correct\"\n",
    "assert len(preprocessed_test_sentences) == 1000, \"The number of train sentences is not correct\"\n",
    "assert len(vocab) == 307, \"Error in the number of vocabulary\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f2a175",
   "metadata": {
    "id": "48f2a175"
   },
   "source": [
    "# Part two:\n",
    "In this part, You will build an Add one (La place) Smoothed bi-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6e0664c6",
   "metadata": {
    "id": "6e0664c6"
   },
   "outputs": [],
   "source": [
    "class BigramLM:\n",
    "\n",
    "    # The class constructor takes the vocabulary\n",
    "    # creates a |V| * |V| numpy 2D matrix for the bi-gram model filled with zeros\n",
    "    # It also creates two dictionaries to be used for token identification\n",
    "    def __init__(self, vocab):\n",
    "        self.id2word = {i: word for i, word in enumerate(list(vocab))}\n",
    "        self.word2id = {word: i for i, word in self.id2word.items()}\n",
    "        self.vocab_size = len(vocab)\n",
    "\n",
    "        # create a numpy |V| * |V| 2D array filled with zeros\n",
    "        self.CountsMatrix = np.zeros((self.vocab_size, self.vocab_size), dtype=int)\n",
    "\n",
    "\n",
    "    ####################################### TODO: Complete the BigramLM class ######################################\n",
    "    # This function is responsible for training the Language Model\n",
    "    # It is given the preprocessed training set sentences\n",
    "    # The goal is to fill the 2D matrix with the appropriate counts\n",
    "    # hint: check bigrams() and FreqDist() from nltk\n",
    "    # hint: loop over the sentences to fill the matrix with the appropriate counts\n",
    "    def train(self, train_sentences):\n",
    "        for sentence in train_sentences:\n",
    "            for w1,w2 in bigrams(sentence):\n",
    "                self.CountsMatrix[self.word2id[w1]][self.word2id[w2]] += 1\n",
    "    \n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    # this function takes two words and calculates the Add-one Smoothed bi-gram probability of it\n",
    "    # Of course the function will make use of the 2D counts matrix built while training\n",
    "    # The function assumes that word1 precedes word2\n",
    "    # The function must return the calculated probability\n",
    "    def calcProbability(self, word1, word2):\n",
    "        probablity = (self.CountsMatrix[self.word2id[word1]][self.word2id[word2]] + 1) / (np.sum(self.CountsMatrix[self.word2id[word1]]) + self.vocab_size)\n",
    "        return probablity\n",
    "        \n",
    "\n",
    "\n",
    "    # This function takes a preprocessed tokenized sentence with exactly one token = mask (look at the masked test set)\n",
    "    # The function returns a word from the vocabulary that is more likely to be masked\n",
    "    # hint: calculate the probabilities of all possible words in the vocabulary and return the word with the maximum probability\n",
    "    def predict(self, tokenized_sentence):\n",
    "        index=0\n",
    "        for i in range(len(tokenized_sentence)):\n",
    "            if tokenized_sentence[i] == \"mask\":\n",
    "                index = i-1\n",
    "                break\n",
    "                \n",
    "        \n",
    "        probablities = []\n",
    "        \n",
    "        for word in self.id2word.values():\n",
    "            if word == \"mask\":\n",
    "                continue\n",
    "            \n",
    "            probablities.append(self.calcProbability(tokenized_sentence[index], word))\n",
    "           \n",
    "        return self.id2word[np.argmax(probablities)]\n",
    "        \n",
    "\n",
    "\n",
    "    # This function takes a token and samples the next token\n",
    "    # hint: check np.random.choice\n",
    "    def sample(self, token):\n",
    "        probablities = []\n",
    "        for i in range(len(self.id2word)):\n",
    "            probablities.append(self.calcProbability(token, self.id2word[i]))\n",
    "            \n",
    "        return np.random.choice(list(self.id2word.values()), 1, p=probablities)[0]\n",
    "\n",
    "\n",
    "    # This function generates a new order using the Bi-gram probabilities\n",
    "    # Returns one string where the generated tokens are white space separated\n",
    "    # Note: The start token <s> and end token </s> shouldn't appear in the generated order\n",
    "    # hint: start the generation with <s> and stop generating tokens when you reach </s>\n",
    "    def generateOrder(self):\n",
    "        sentence=\"\"\n",
    "        token='<s>'\n",
    "        while token != '</s>':\n",
    "            token = self.sample(token)\n",
    "            if token != '</s>':\n",
    "                sentence += token + \" \"\n",
    "        return sentence\n",
    "    ################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b4632",
   "metadata": {
    "id": "be7b4632"
   },
   "source": [
    "# The next cell to make sure your Bigram LM is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e9cba284",
   "metadata": {
    "id": "e9cba284",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bigramLM = BigramLM(vocab)\n",
    "bigramLM.train(preprocessed_train_sentences)\n",
    "\n",
    "assert bigramLM.calcProbability('i', \"'d\") == 0.6489202480222365, \"Probability Error\"\n",
    "assert bigramLM.calcProbability(\"'d\", \"i\") == 0.000299311583358276, \"Probability Error\"\n",
    "assert bigramLM.predict(['<s>', 'one', 'mask', 'sized', 'green', 'pepper', '</s>']) == 'party', \"Prediction Error\"\n",
    "gerated_order = bigramLM.generateOrder()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e67454a",
   "metadata": {
    "id": "0e67454a"
   },
   "source": [
    "# Part three:\n",
    "In this part, you will build and RNN Language model from scratch. This part may be a little bit tricky specially in the back propagation, but we will go trough it step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d5ead671",
   "metadata": {
    "id": "d5ead671"
   },
   "outputs": [],
   "source": [
    "class RNNLM:\n",
    "\n",
    "    # The class constructor takes the vocabulary\n",
    "    # creates two dictionaries to be used for token identification\n",
    "    # creates the embeddings matrix (one hot encodings for tokens)\n",
    "    # creates all wights of the RNN Wx, Wh, b, Wy, by\n",
    "    def __init__(self, vocab, hidden_dim=128):\n",
    "        vocab.add('null')\n",
    "        self.id2word = {i: word for i, word in enumerate(list(vocab))}\n",
    "        self.word2id = {word: i for i, word in self.id2word.items()}\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.null_word_index = self.word2id['null']\n",
    "\n",
    "        # Create a numpy |V| * |V| 2D identity array\n",
    "        self.embeddings = np.identity(self.vocab_size)\n",
    "\n",
    "        # Initialize RNN weights\n",
    "        np.random.seed(5)\n",
    "        self.Wx = np.random.randn(self.vocab_size, hidden_dim).astype(np.float64)\n",
    "        self.Wx /= np.sqrt(self.vocab_size)\n",
    "        self.Wh = np.random.randn(hidden_dim, hidden_dim).astype(np.float64)\n",
    "        self.Wh /= np.sqrt(hidden_dim)\n",
    "        self.b = np.zeros(hidden_dim).astype(np.float64)\n",
    "        self.Wy = np.random.randn(hidden_dim, self.vocab_size).astype(np.float64)\n",
    "        self.Wy /= np.sqrt(hidden_dim)\n",
    "        self.by = np.zeros(self.vocab_size).astype(np.float64)\n",
    "\n",
    "\n",
    "    def one_step_forward(self, x, h_prev, Wx, Wh, b):\n",
    "        \"\"\"\n",
    "        This function runs one time step of the RNN. It should implement the RNN equation\n",
    "        that takes input embedding and the previous hidden state then produces the new hidden state.\n",
    "\n",
    "        Inputs:\n",
    "        - x: Input data of the current time step of shape (N, D)\n",
    "        where N is the number of tokens and D is the vocab size\n",
    "        - h_prev: the hidden state from the previous time step of shape (N, H)\n",
    "        - Wx: the weight matrix for input to hidden transformation of shape (D, H)\n",
    "        - Wh: the weight matrix for hidden to hidden tranformation of shape (H, H)\n",
    "        - b: the bias of shape (H,)\n",
    "\n",
    "        Returns a tuple of:\n",
    "        - next_h: Next hidden state of shape (N, H)\n",
    "        - cache: a tupple of all data needed for backpropagation\n",
    "        \"\"\"\n",
    "        next_h, cache = None, None\n",
    "\n",
    "        ############################### TODO: Impelement RNN equation for forward pass ##############################\n",
    "\n",
    "        # use numpy to implement this equation next_h = tanh(Wx*x + Wh*h_prev + b)\n",
    "        next_h = np.tanh(np.dot(x, Wx) + np.dot(h_prev, Wh) + b)\n",
    "\n",
    "        # cache all needed values for backpropagation\n",
    "        cache = (x, h_prev, Wx, Wh,b, next_h) # maybe modified\n",
    "\n",
    "        #############################################################################################################\n",
    "\n",
    "        return next_h, cache\n",
    "\n",
    "\n",
    "    def one_step_backward(self, dnext_h, cache):\n",
    "        \"\"\"\n",
    "        This function implements the backward pass of a single time step RNN\n",
    "\n",
    "        Inputs:\n",
    "        - dnext_h: the gradient of the loss with respect to the next hidden state of shape (N, H)\n",
    "        - cache: a tupple that we cached before from the forward pass\n",
    "\n",
    "        Returns:\n",
    "        - dprev_h: Gradients of previous hidden state, of shape (N, H)\n",
    "        - dWx: Gradients of input-to-hidden weights, of shape (D, H)\n",
    "        - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)\n",
    "        - db: Gradients of bias vector, of shape (H,)\n",
    "        \"\"\"\n",
    "        dprev_h, dWx, dWh, db = None, None, None, None\n",
    "        x, prev_h, Wx, Wh, b, tanh = cache\n",
    "\n",
    "        ################# TODO: Impelement the backward pass by calculating derivatives and applying chain rule ###########\n",
    "        dloss_dat = dnext_h * (1 - tanh**2)\n",
    "        dWx = np.dot(x.T, dloss_dat)\n",
    "        dWh = np.dot(prev_h.T, dloss_dat)\n",
    "        db=np.sum(dloss_dat, axis=0)\n",
    "        dprev_h = np.dot(dloss_dat, Wh.T)\n",
    "        \n",
    "\n",
    "\n",
    "        ###################################################################################################################\n",
    "\n",
    "        return dprev_h, dWx, dWh, db\n",
    "\n",
    "\n",
    "    def full_forward_pass(self, x, h0, Wx, Wh, b):\n",
    "        \"\"\"\n",
    "        This function runs the RNN forward on an entire sequence of data. We assume an input\n",
    "        sequence composed of T vectors, each of dimension D. The RNN uses a hidden\n",
    "        size of H, and we work over a minibatch containing N sequences. After running\n",
    "        the RNN forward, we return the hidden states for all timesteps.\n",
    "\n",
    "        Inputs:\n",
    "        - x: Input data for the entire timeseries, of shape (N, T, D).\n",
    "        - h0: Initial hidden state, of shape (N, H)\n",
    "        - Wx: Weight matrix for input to hidden transformation, of shape (D, H)\n",
    "        - Wh: Weight matrix for hidden to hidden transformation, of shape (H, H)\n",
    "        - b: Biases of shape (H,)\n",
    "\n",
    "        Returns a tuple of:\n",
    "        - h: Hidden states for the entire timeseries, of shape (N, T, H).\n",
    "        - cache: All values needed in the backward pass\n",
    "        \"\"\"\n",
    "        h, cache = None, None\n",
    "        N, T, D = x.shape\n",
    "        H = h0.shape[1]\n",
    "        #################### TODO: Implement the full forward pass of the RNN ###########################\n",
    "        # Hint 1: You will use the one_step_forward to compute each time step hidden state and cache\n",
    "        # Hint 2: For loop over T and call one_step_forward with the appropriate parameters\n",
    "        Allcache = []\n",
    "        h=np.zeros((N, T, H))\n",
    "        for t in range(T):\n",
    "            if t == 0:\n",
    "                h[:, t, :], cache = self.one_step_forward(x[:, t, :], h0, Wx, Wh, b)\n",
    "            else:\n",
    "                h[:, t, :], cache = self.one_step_forward(x[:, t, :], h[:, t-1, :], Wx, Wh, b)\n",
    "                \n",
    "            Allcache.append(cache)\n",
    "        # create numpy array filled with zeros of shape (N, T, H) for h to be filled then returned\n",
    "\n",
    "\n",
    "        # initialize the cache as an empty list to be filled with all caches returned from one_step_forward\n",
    "\n",
    "\n",
    "        # loop over T and calculate the needed hidden states and caches\n",
    "\n",
    "\n",
    "        #################################################################################################\n",
    "\n",
    "        return h, Allcache\n",
    "\n",
    "\n",
    "    def full_backward_pass(self, dh, cache):\n",
    "        \"\"\"\n",
    "        This function computes the backward pass for the RNN over an entire sequence of data.\n",
    "        Inputs:\n",
    "        - dh: The loss gradient with respect to all hidden states, of shape (N, T, H).\n",
    "\n",
    "        Returns a tuple of:\n",
    "        - dh0: Gradient of initial hidden state, of shape (N, H)\n",
    "        - dWx: Gradient of input to hidden weights, of shape (D, H)\n",
    "        - dWh: Gradient of hidden to hidden weights, of shape (H, H)\n",
    "        - db: Gradient of biases, of shape (H,)\n",
    "        \"\"\"\n",
    "        dh0, dWx, dWh, db = None, None, None, None\n",
    "        N, T, H = dh.shape\n",
    "\n",
    "        #################### TODO: Implement the full backward pass of the RNN ###########################\n",
    "        # Hint 1: You will use the one_step_backward to compute each time step gradients\n",
    "        # Hint 2: For loop over T and call one_step_backward with the appropriate parameters\n",
    "        # Hint 3: Don't forget to add dprev_h returned from one_step_backward to the appropriate index in dh while passing\n",
    "        #         to one_step_backward\n",
    "        # print(type(cache))\n",
    "\n",
    "       \n",
    "        for t in reversed(range(T)):\n",
    "            if t == T-1:\n",
    "                dh0, dWx, dWh, db = self.one_step_backward(dh[:, t, :], cache[t])\n",
    "                # dht=dh0\n",
    "            else:\n",
    "                dh0, dWxt, dWht, dbt = self.one_step_backward(dh0 + dh[:, t, :], cache[t])\n",
    "                dWx += dWxt\n",
    "                dWh += dWht\n",
    "                db += dbt\n",
    "                # dh0 += dht\n",
    "\n",
    "        ##################################################################################################\n",
    "\n",
    "        return dh0, dWx, dWh, db\n",
    "\n",
    "    def hidden_to_scores_forward(self, h, Wy, by):\n",
    "        \"\"\"\n",
    "        This function implements the forward pass of transforming the hidden state to the scores vector.\n",
    "        The input is a set of H-dimensional vectors arranged into a minibatch of N timeseries, each of length T.\n",
    "        The goal is transform from the hidden state to the scores output vector with size V (number of vocabulary).\n",
    "\n",
    "        Inputs:\n",
    "        - h: Input data of shape (N, T, H)\n",
    "        - Wy: Weights of shape (H, V)\n",
    "        - by: Biases of shape (V,)\n",
    "\n",
    "        Returns a tuple of:\n",
    "        - y: Output data of shape (N, T, V)\n",
    "        - cache: Values needed for the backward pass\n",
    "        \"\"\"\n",
    "        N, T, H = h.shape\n",
    "        V = by.shape[0]\n",
    "\n",
    "        y, cache = None, None\n",
    "\n",
    "        ##################### TODO: calculate the scores matrix y and cache all needed values ########################\n",
    "        # Hint 1: you need to caculate this formula y = Wy*h + by\n",
    "        y=np.dot(h.reshape(N*T, H), Wy) \n",
    "        y=y.reshape(N,T,V)\n",
    "        y=y+by.T\n",
    "        cache = (h, Wy, by, y)\n",
    "        # Hint 2: you can reshape h from (N, T, H) to (N*T, H) before the matrix multiplication\n",
    "        #         then reshape it back to (N, T, H) after the matrix multiplication and before adding the by\n",
    "\n",
    "\n",
    "        ##############################################################################################################\n",
    "\n",
    "        return y, cache\n",
    "\n",
    "\n",
    "    def hidden_to_scores_backward(self, dy, cache):\n",
    "        \"\"\"\n",
    "        Backward pass for the hidden to scores layer.\n",
    "        Input:\n",
    "        - dy: The gradients of shape (N, T, V)\n",
    "        - cache: Values from forward pass\n",
    "        Returns a tuple of:\n",
    "        - dh: Gradient of input, of shape (N, T, H)\n",
    "        - dWy: Gradient of weights, of shape (H, V)\n",
    "        - dby: Gradient of biases, of shape (V,)\n",
    "        \"\"\"\n",
    "        h, Wy, by, y = cache\n",
    "        N, T, H = h.shape\n",
    "        V = by.shape[0]\n",
    "\n",
    "        dh, dWy, dby = None, None, None\n",
    "\n",
    "        ################################ TODO: calculate the gradients ##############################\n",
    "        # Hint: use reshape to ensure correct dimensions for matrix multiplications and derivat\n",
    "        dh=np.dot(dy.reshape(N*T,V), Wy.T)\n",
    "        dh=dh.reshape(N,T,H)\n",
    "        dWy=np.dot(h.reshape(N*T, H).T, dy.reshape(N*T,V))\n",
    "        dby=np.sum(dy, axis=(0,1))\n",
    "        \n",
    "        \n",
    "        #############################################################################################\n",
    "\n",
    "        return dh, dWy, dby\n",
    "\n",
    "    def softmax_loss(self, x, y, mask):\n",
    "        \"\"\"\n",
    "        This function calculates the softmax loss over minibatches of size N.\n",
    "\n",
    "        Inputs:\n",
    "        - x: Input scores, of shape (N, T, V)\n",
    "        - y: Ground-truth indices, of shape (N, T) where each element is in the range\n",
    "             0 <= y[i, t] < V\n",
    "        - mask: Boolean array of shape (N, T) where mask[i, t] tells whether or not\n",
    "          the scores at x[i, t] should contribute to the loss.\n",
    "        Returns a tuple of:\n",
    "        - loss: Scalar giving loss\n",
    "        - dx: Gradient of loss with respect to scores x, of shape (N, T, V).\n",
    "        \"\"\"\n",
    "        N, T, V = x.shape\n",
    "        x_flat = x.reshape(N * T, V)\n",
    "        y_flat = y.reshape(N * T)\n",
    "        mask_flat = mask.reshape(N * T)\n",
    "        probs = np.exp(x_flat - np.max(x_flat, axis=1, keepdims=True))\n",
    "        probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "        loss = -np.sum(mask_flat * np.log(probs[np.arange(N * T), y_flat])) / N\n",
    "        dx_flat = probs.copy()\n",
    "        dx_flat[np.arange(N * T), y_flat] -= 1\n",
    "        dx_flat /= N\n",
    "        dx_flat *= mask_flat[:, None]\n",
    "        dx = dx_flat.reshape(N, T, V)\n",
    "        return loss, dx\n",
    "\n",
    "\n",
    "    def word_embeddings_forward(self, x_sentences, W_embedd):\n",
    "        \"\"\"\n",
    "        This function takes input with tokens as indexes to produce their word embeddings\n",
    "\n",
    "        Inputs:\n",
    "        - x_sentences: input sentences of shape (N, T)\n",
    "        - W_embedd: embeddings matrix of size (V, V)\n",
    "\n",
    "        Returns:\n",
    "        - x: matrix with word embeddings of shape (N, T, V)\n",
    "        \"\"\"\n",
    "        return W_embedd[x_sentences]\n",
    "\n",
    "\n",
    "    def loss(self, sentences):\n",
    "        \"\"\"\n",
    "        This function computes the loss for training and the gradients of all RNN weights.\n",
    "\n",
    "        Inputs:\n",
    "        - sentences: Ground-truth sentences; an integer array of shape (N, T) where\n",
    "          each element is in the range 0 <= y[i, t] < V\n",
    "        Returns a tuple of:\n",
    "        - loss: Scalar loss\n",
    "        - grads: Dictionary of gradients for all RNN weights\n",
    "        \"\"\"\n",
    "        # Since we are training a language model, for each token input to the RNN we should predict the\n",
    "        # next token. So the inputs will the all tokens from begin to end -1 and the outputs will be all tokens\n",
    "        # except the first token\n",
    "        sentences_in = sentences[:, :-1]\n",
    "        sentences_out = sentences[:, 1:]\n",
    "\n",
    "        # You'll need this\n",
    "        mask = (sentences_out != self.null_word_index)\n",
    "\n",
    "        # Word embedding matrix\n",
    "        W_embed = self.embeddings\n",
    "\n",
    "        # Input-to-hidden, hidden-to-hidden, and biases for the RNN\n",
    "        Wx, Wh, b = self.Wx, self.Wh, self.b\n",
    "\n",
    "        # Weight and bias for the hidden-to-vocab transformation.\n",
    "        Wy, by = self.Wy, self.by\n",
    "\n",
    "        # sizes\n",
    "        N, T = sentences.shape\n",
    "        V, H = Wx.shape\n",
    "\n",
    "        loss, grads = 0.0, {'Wx':None, 'Wh':None, 'b':None, 'Wy':None, 'by':None}\n",
    "\n",
    "        ###################### TODO: Implement the forward and backward passes for the RNN LM #########################\n",
    "        # In the forward pass you will need to do the following:\n",
    "\n",
    "        # (1) Initilize the initial hidden state h0 to a zeros matrix of size (N, H)\n",
    "        h0=np.zeros((N, H))\n",
    "\n",
    "        # (2) Use a word embedding layer to transform the words in captions_in\n",
    "        #     from indices to vectors, giving an array of shape (N, T, W).\n",
    "        x=self.word_embeddings_forward(sentences_in, W_embed)\n",
    "\n",
    "        # (3) Call rnn_forward with the appropriate parameters to produce\n",
    "        #     array of hidden states with shape (N, T, H)\n",
    "\n",
    "        h, cache = self.full_forward_pass(x, h0, Wx, Wh, b)\n",
    "        # (4) Call hidden_to_scores_forward to get an array of scores of shape (N, T, V)\n",
    "        y, cache_voc = self.hidden_to_scores_forward(h, Wy, by)\n",
    "\n",
    "        # (5) Call softmax_loss to compute loss using captions_out, ignoring\n",
    "        #     the points where the output word is <NULL> using the mask above.\n",
    "\n",
    "        loss, dx = self.softmax_loss(y, sentences_out, mask)\n",
    "\n",
    "        # In the backward pass you will need to compute the gradient of the loss\n",
    "        # with respect to all model parameters. Use the loss and grads variables\n",
    "        # defined above to store loss and gradients;  grads['k'] should give the\n",
    "        # gradients for self..\n",
    "\n",
    "        # (1) Call hidden_to_scores_backward\n",
    "        dh, dWy, dby= self.hidden_to_scores_backward(dx, cache_voc)\n",
    "\n",
    "        # (2) Call full_backward_pass\n",
    "        dh0, dWx, dWh, db = self.full_backward_pass(dh, cache)\n",
    "\n",
    "        # (3) save the gradients in the grads dictionary\n",
    "        grads['Wx'] = dWx\n",
    "        grads['Wh'] = dWh\n",
    "        grads['b'] = db\n",
    "        grads['Wy'] = dWy\n",
    "        grads['by'] = dby\n",
    "\n",
    "        ###############################################################################################################\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    def preprocessDataset(self, train_set):\n",
    "        train_set = [[self.word2id[word] for word in s] for s in train_set]\n",
    "        max_length = np.max([len(s) for s in train_set])\n",
    "        train_set = [s + [self.null_word_index] * (max_length - len(s)) for s in train_set]\n",
    "        return np.array(train_set, dtype=int)\n",
    "\n",
    "    def train(self, sentences, batch_size=64, num_iterations=100, lr=0.0001):\n",
    "        for iteration in range(num_iterations):\n",
    "            index = 0\n",
    "            while index < len(sentences):\n",
    "                loss, grads = self.loss(sentences[index: index + batch_size])\n",
    "                self.Wx -= lr * grads['Wx']\n",
    "                self.Wh -= lr * grads['Wh']\n",
    "                self.b -= lr * grads['b']\n",
    "                self.Wy -= lr * grads['Wy']\n",
    "                self.by -= lr * grads['by']\n",
    "                index += batch_size\n",
    "            print(f\"Iteration: {iteration} | loss = {loss}\")\n",
    "\n",
    "    def predict(self, tokenized_sentence):\n",
    "        idx = tokenized_sentence.index(\"mask\")\n",
    "        prev_h = np.zeros((1, self.Wh.shape[0]), dtype=np.float64)\n",
    "        x = self.word_embeddings_forward([self.word2id[token] for token in tokenized_sentence[:idx]], self.embeddings).reshape(1, idx, self.vocab_size)\n",
    "        for i in range(idx):\n",
    "            prev_h, cache_h = self.one_step_forward(x[:,i,:], prev_h, self.Wx, self.Wh, self.b)\n",
    "        out, cache_voc = self.hidden_to_scores_forward(prev_h.reshape(1, 1, -1), self.Wy, self.by)\n",
    "        return self.id2word[np.argmax(out)]\n",
    "\n",
    "\n",
    "    def sample(self, token, prev_h):\n",
    "        \"\"\"\n",
    "        This function takes a token and previous hidden state and samples the next token based on the softmax\n",
    "        Probability distribution\n",
    "\n",
    "        Inputs:\n",
    "        - token: string containing the current token\n",
    "        - prev_h: the previous hidden state of shape (1, H)\n",
    "\n",
    "        Returns:\n",
    "        - next_token: string containing the sampled next token\n",
    "        - curr_h: the produced hidden state from the RNN\n",
    "        \"\"\"\n",
    "        x = self.word_embeddings_forward([self.word2id[token]], self.embeddings).reshape(1, 1, self.vocab_size)\n",
    "        Wx, Wh, b = self.Wx, self.Wh, self.b\n",
    "        Wy, by = self.Wy, self.by\n",
    "\n",
    "        next_token, curr_h = None, None\n",
    "        ############################ TODO: implement the sampling ############################################\n",
    "        # Call one_step_forward with the appropriate parameters\n",
    "        curr_h, cache =self.one_step_forward(x, prev_h, Wx, Wh, b) \n",
    "        # Call hidden_to_scores_forward to produce the scores\n",
    "        y, cache_voc = self.hidden_to_scores_forward(curr_h, Wy, by)\n",
    "        # Compute the softmax yourself. Hint: check np.exp\n",
    "        prob=[]\n",
    "        \n",
    "        \n",
    "        for i in y.reshape(-1):\n",
    "            prob.append(np.exp(i))\n",
    "        prob=prob/np.sum(prob)\n",
    "        # Sample the token using np.random.choice\n",
    "        next_token = np.random.choice(list(self.id2word.values()), 1, p=prob)[0]\n",
    "        ######################################################################################################\n",
    "        return next_token, curr_h\n",
    "\n",
    "    def generateOrder(self):\n",
    "        prev_h = np.zeros((1, self.Wh.shape[0]), dtype=np.float64)\n",
    "        tokens = []\n",
    "        t = '<s>'\n",
    "        while t != '</s>':\n",
    "            t, prev_h = self.sample(t, prev_h)\n",
    "            tokens.append(t)\n",
    "        return ' '.join(tokens[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f0e26d",
   "metadata": {
    "id": "33f0e26d"
   },
   "source": [
    "# Let's Test Your Code\n",
    "The next code cell implements functions for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "03b1f075",
   "metadata": {
    "id": "03b1f075"
   },
   "outputs": [],
   "source": [
    "def rel_error(x, y):\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
    "    \"\"\"\n",
    "    a naive implementation of numerical gradient of f at x\n",
    "    - f should be a function that takes a single argument\n",
    "    - x is the point (numpy array) to evaluate the gradient at\n",
    "    \"\"\"\n",
    "\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print(ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25645083",
   "metadata": {
    "id": "25645083"
   },
   "source": [
    "# Test One Step Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ea6e4d37",
   "metadata": {
    "id": "ea6e4d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_h error:  6.292421426471037e-09\n"
     ]
    }
   ],
   "source": [
    "N, D, H = 3, 10, 4\n",
    "rnnLM = RNNLM(vocab)\n",
    "x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n",
    "prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.2, 0.4, num=H)\n",
    "\n",
    "next_h, _ = rnnLM.one_step_forward(x, prev_h, Wx, Wh, b)\n",
    "expected_next_h = np.asarray([\n",
    "  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
    "  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n",
    "  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n",
    "\n",
    "assert rel_error(expected_next_h, next_h) < 1e-8, \"Error in one_step_forward\"\n",
    "print('next_h error: ', rel_error(expected_next_h, next_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea924f7f",
   "metadata": {
    "id": "ea924f7f"
   },
   "source": [
    "# Test One Step Backwad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a00b1fb6",
   "metadata": {
    "id": "a00b1fb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dprev_h error:  2.5658753744307165e-10\n",
      "dWx error:  8.845193511067368e-10\n",
      "dWh error:  1.6263476408377522e-10\n",
      "db error:  4.584901020876022e-11\n"
     ]
    }
   ],
   "source": [
    "rnnLM = RNNLM(vocab)\n",
    "np.random.seed(231)\n",
    "N, D, H = 4, 5, 6\n",
    "x = np.random.randn(N, D)\n",
    "h = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "\n",
    "out, cache = rnnLM.one_step_forward(x, h, Wx, Wh, b)\n",
    "\n",
    "dnext_h = np.random.randn(*out.shape)\n",
    "\n",
    "fx = lambda x: rnnLM.one_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fh = lambda prev_h: rnnLM.one_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnnLM.one_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnnLM.one_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnnLM.one_step_forward(x, h, Wx, Wh, b)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dnext_h)\n",
    "dprev_h_num = eval_numerical_gradient_array(fh, h, dnext_h)\n",
    "dWx_num = eval_numerical_gradient_array(fWx, Wx, dnext_h)\n",
    "dWh_num = eval_numerical_gradient_array(fWh, Wh, dnext_h)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dnext_h)\n",
    "\n",
    "dprev_h, dWx, dWh, db = rnnLM.one_step_backward(dnext_h, cache)\n",
    "\n",
    "assert rel_error(dprev_h_num, dprev_h) < 1e-9, \"Error in one_step_backward\"\n",
    "assert rel_error(dWx_num, dWx) < 1e-9, \"Error in one_step_backward\"\n",
    "assert rel_error(dWh_num, dWh) < 1e-9, \"Error in one_step_backward\"\n",
    "assert rel_error(db_num, db) < 1e-9, \"Error in one_step_backward\"\n",
    "print('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc0f72",
   "metadata": {
    "id": "00bc0f72"
   },
   "source": [
    "# Test full forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "e8ecd42f",
   "metadata": {
    "id": "e8ecd42f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h error:  7.728466180186066e-08\n"
     ]
    }
   ],
   "source": [
    "N, T, D, H = 2, 3, 4, 5\n",
    "rnnLM = RNNLM(vocab)\n",
    "\n",
    "x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n",
    "h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.7, 0.1, num=H)\n",
    "\n",
    "h, _ = rnnLM.full_forward_pass(x, h0, Wx, Wh, b)\n",
    "expected_h = np.asarray([\n",
    "  [\n",
    "    [-0.42070749, -0.27279261, -0.11074945,  0.05740409,  0.22236251],\n",
    "    [-0.39525808, -0.22554661, -0.0409454,   0.14649412,  0.32397316],\n",
    "    [-0.42305111, -0.24223728, -0.04287027,  0.15997045,  0.35014525],\n",
    "  ],\n",
    "  [\n",
    "    [-0.55857474, -0.39065825, -0.19198182,  0.02378408,  0.23735671],\n",
    "    [-0.27150199, -0.07088804,  0.13562939,  0.33099728,  0.50158768],\n",
    "    [-0.51014825, -0.30524429, -0.06755202,  0.17806392,  0.40333043]]])\n",
    "assert rel_error(expected_h, h) < 1e-7, \"Error in full forward pass\"\n",
    "print('h error: ', rel_error(expected_h, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d21145",
   "metadata": {
    "id": "25d21145"
   },
   "source": [
    "# Test full backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b37b6669",
   "metadata": {
    "id": "b37b6669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dh0 error:  6.453285937992153e-11\n",
      "dWx error:  4.642588113556617e-10\n",
      "dWh error:  8.14374529514666e-10\n",
      "db error:  3.9550822355654096e-11\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "rnnLM = RNNLM(vocab)\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "\n",
    "x = np.random.randn(N, T, D)\n",
    "h0 = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "\n",
    "out, cache = rnnLM.full_forward_pass(x, h0, Wx, Wh, b)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "\n",
    "dh0, dWx, dWh, db = rnnLM.full_backward_pass(dout, cache)\n",
    "\n",
    "fx = lambda x: rnnLM.full_forward_pass(x, h0, Wx, Wh, b)[0]\n",
    "fh0 = lambda h0: rnnLM.full_forward_pass(x, h0, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnnLM.full_forward_pass(x, h0, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnnLM.full_forward_pass(x, h0, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnnLM.full_forward_pass(x, h0, Wx, Wh, b)[0]\n",
    "\n",
    "dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n",
    "dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n",
    "dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
    "\n",
    "assert rel_error(dh0_num, dh0) < 1e-9, \"Error in full_backward_pass\"\n",
    "assert rel_error(dWx_num, dWx) < 1e-9, \"Error in full_backward_pass\"\n",
    "assert rel_error(dWh_num, dWh) < 1e-9, \"Error in full_backward_pass\"\n",
    "assert rel_error(db_num, db) < 1e-9, \"Error in full_backward_pass\"\n",
    "\n",
    "print('dh0 error: ', rel_error(dh0_num, dh0))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420a0c7",
   "metadata": {
    "id": "4420a0c7"
   },
   "source": [
    "# Check hidden to scores forward and backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "97aa4935",
   "metadata": {
    "id": "97aa4935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  6.848770688344809e-10\n",
      "dw error:  8.503919884552969e-11\n",
      "db error:  7.534133932432161e-12\n"
     ]
    }
   ],
   "source": [
    "# Gradient check for temporal affine layer\n",
    "N, T, D, M = 2, 3, 4, 5\n",
    "rnnLM = RNNLM(vocab)\n",
    "x = np.random.randn(N, T, D)\n",
    "w = np.random.randn(D, M)\n",
    "b = np.random.randn(M)\n",
    "\n",
    "out, cache = rnnLM.hidden_to_scores_forward(x, w, b)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "\n",
    "fx = lambda x: rnnLM.hidden_to_scores_forward(x, w, b)[0]\n",
    "fw = lambda w: rnnLM.hidden_to_scores_forward(x, w, b)[0]\n",
    "fb = lambda b: rnnLM.hidden_to_scores_forward(x, w, b)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "dw_num = eval_numerical_gradient_array(fw, w, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
    "\n",
    "dx, dw, db = rnnLM.hidden_to_scores_backward(dout, cache)\n",
    "\n",
    "assert rel_error(dx_num, dx), \"Error in hidden to scores\"\n",
    "assert rel_error(dw_num, dw), \"Error in hidden to scores\"\n",
    "assert rel_error(db_num, db), \"Error in hidden to scores\"\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d199e23a",
   "metadata": {
    "id": "d199e23a"
   },
   "source": [
    "# Check the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "0ba3e645",
   "metadata": {
    "id": "0ba3e645"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wx relative error: 3.110372e-08\n",
      "Wh relative error: 9.356980e-07\n",
      "b relative error: 1.915599e-08\n",
      "Wy relative error: 3.973081e-08\n",
      "by relative error: 4.594465e-10\n"
     ]
    }
   ],
   "source": [
    "N, H = 5, 20\n",
    "test_vocab = {'', 'cat', 'dog'}\n",
    "V = len(test_vocab)\n",
    "T = 10\n",
    "\n",
    "model = RNNLM(test_vocab, H)\n",
    "\n",
    "sentences_test = (np.arange(N * T) % V).reshape(N, T)\n",
    "\n",
    "loss, grads = model.loss(sentences_test)\n",
    "\n",
    "\n",
    "f = lambda _: model.loss(sentences_test)[0]\n",
    "param_grad_num = eval_numerical_gradient(f, model.Wx, verbose=False, h=1e-6)\n",
    "e = rel_error(param_grad_num, grads['Wx'])\n",
    "print('%s relative error: %e' % ('Wx', e))\n",
    "assert e < 1e-6, \"Error in dWx\"\n",
    "\n",
    "param_grad_num = eval_numerical_gradient(f, model.Wh, verbose=False, h=1e-6)\n",
    "e = rel_error(param_grad_num, grads['Wh'])\n",
    "print('%s relative error: %e' % ('Wh', e))\n",
    "assert e < 1e-6, \"Error in dWh\"\n",
    "\n",
    "param_grad_num = eval_numerical_gradient(f, model.b, verbose=False, h=1e-6)\n",
    "e = rel_error(param_grad_num, grads['b'])\n",
    "print('%s relative error: %e' % ('b', e))\n",
    "assert e < 1e-6, \"Error in db\"\n",
    "\n",
    "param_grad_num = eval_numerical_gradient(f, model.Wy, verbose=False, h=1e-6)\n",
    "e = rel_error(param_grad_num, grads['Wy'])\n",
    "print('%s relative error: %e' % ('Wy', e))\n",
    "assert e < 1e-4, \"Error in dWy\"\n",
    "\n",
    "param_grad_num = eval_numerical_gradient(f, model.by, verbose=False, h=1e-6)\n",
    "e = rel_error(param_grad_num, grads['by'])\n",
    "print('%s relative error: %e' % ('by', e))\n",
    "assert e < 1e-6, \"Error in dby\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c23262",
   "metadata": {
    "id": "60c23262"
   },
   "source": [
    "# The following cells will produce the output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a08d7556",
   "metadata": {
    "id": "a08d7556"
   },
   "outputs": [],
   "source": [
    "with open(f'{team_ID}.txt', 'w') as f:\n",
    "    f.write(team_ID + '\\n')\n",
    "    f.write(Student1_Name + '\\n')\n",
    "    f.write(Student2_Name + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "4d10eec2",
   "metadata": {
    "id": "4d10eec2"
   },
   "outputs": [],
   "source": [
    "bigramLM = BigramLM(vocab)\n",
    "bigramLM.train(preprocessed_train_sentences)\n",
    "\n",
    "predictions_bigram = []\n",
    "for sentence in preprocessed_test_sentences:\n",
    "    predictions_bigram.append(bigramLM.predict(sentence))\n",
    "\n",
    "pd.DataFrame({'predictions': predictions_bigram}).to_csv(f'{team_ID}_bigram_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "6c2ba40e",
   "metadata": {
    "id": "6c2ba40e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 | loss = 59.4416993021273\n",
      "Iteration: 1 | loss = 52.37730660624932\n",
      "Iteration: 2 | loss = 45.83795974093414\n",
      "Iteration: 3 | loss = 39.99168641636873\n",
      "Iteration: 4 | loss = 36.9319767502313\n",
      "Iteration: 5 | loss = 33.58164152718128\n",
      "Iteration: 6 | loss = 31.71998735471281\n",
      "Iteration: 7 | loss = 30.173582010093625\n",
      "Iteration: 8 | loss = 28.879326166665912\n",
      "Iteration: 9 | loss = 27.767569652534213\n",
      "Iteration: 10 | loss = 26.852391174339573\n",
      "Iteration: 11 | loss = 26.076735953937934\n",
      "Iteration: 12 | loss = 25.40650217455236\n",
      "Iteration: 13 | loss = 24.823741948623336\n",
      "Iteration: 14 | loss = 24.310431986545264\n",
      "Iteration: 15 | loss = 23.852144170822005\n",
      "Iteration: 16 | loss = 23.437466611250898\n",
      "Iteration: 17 | loss = 23.057238226890775\n",
      "Iteration: 18 | loss = 22.70441392896457\n",
      "Iteration: 19 | loss = 22.374001154798744\n"
     ]
    }
   ],
   "source": [
    "rnnLM = RNNLM(vocab)\n",
    "train_set_RNN = rnnLM.preprocessDataset(preprocessed_train_sentences)\n",
    "rnnLM.train(train_set_RNN, lr=0.01, num_iterations=20)\n",
    "\n",
    "predictions_rnn = []\n",
    "for sentence in preprocessed_test_sentences:\n",
    "    predictions_rnn.append(rnnLM.predict(sentence))\n",
    "\n",
    "pd.DataFrame({'predictions': predictions_rnn}).to_csv(f'{team_ID}_rnn_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "5c888167",
   "metadata": {
    "id": "5c888167"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram prediction Accuracy =  0.398\n",
      "RNN prediction Accuracy =  0.456\n"
     ]
    }
   ],
   "source": [
    "gold_labels = pd.read_csv('labels.csv')\n",
    "bigram_predictions = pd.read_csv(f'{team_ID}_bigram_predictions.csv')\n",
    "rnn_predictions = pd.read_csv(f'{team_ID}_rnn_predictions.csv')\n",
    "\n",
    "bigram_acc = np.where(bigram_predictions['predictions'] == gold_labels['labels'], True, False)\n",
    "bigram_acc = np.sum(bigram_acc) / len(bigram_acc)\n",
    "\n",
    "rnn_acc = np.where(rnn_predictions['predictions'] == gold_labels['labels'], True, False)\n",
    "rnn_acc = np.sum(rnn_acc) / len(rnn_acc)\n",
    "\n",
    "print(\"Bigram prediction Accuracy = \", bigram_acc)\n",
    "print(\"RNN prediction Accuracy = \", rnn_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c69d50",
   "metadata": {
    "id": "48c69d50"
   },
   "source": [
    "# Let's try generation with our Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "74356f27",
   "metadata": {
    "id": "74356f27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two large diet pepsi five veggies veggies ginger ale and grilled pepper and two 20 fl dried peppers salami 500-ml pineapple sodas peperroni anchovy pesto sixteen onions keto hams \n",
      "\n",
      "a little dried regular perrier fl ounce diet iced teas and kalamata olives pies with mushrooms and five eight ounce diet sprites dish 200 margherita hawaiian tomato sauce \n",
      "\n",
      "i 'd like one pizza with cheeseburger chicago style spicy tunas - size pies with peperonni and one pizza with a lot of green pepper \n",
      "\n",
      "i 'd like three large pizzas with just yorker balzamic glaze high eleven banana pepper fat cheese \n",
      "\n",
      "four large pizzas with peperonni party - black ginger ale and three medium rise high bean chicken and also pulled low spiced have ten 12 oil and one pie also a little stuffed crusts pizza with mozzarella want one party much sausage fourteen pepsis \n",
      "\n",
      "two lunch powder peperronni broccoli dew chickens zeros cans spicy peper and without any chicago hold the thin crust meatballs party seven ups and five party sized pizza with balsamic glaze and also a personal size vegetarian zeros and red onion sauce balzamic sprites \n",
      "\n",
      "personal 10 4 crusts ten stuffed crust \n",
      "\n",
      "two personal waters \n",
      "\n",
      "four pizzas with extra carrots five 500 ml doctor powder alfredo 3 all spiced 200-milliliter cheese and low olive pie with balsamic 4 pepsi apple oz pulled peperonni and three personal zeros carrot supreme york any pineaples any - size thick kalamata olives \n",
      "\n",
      "nine any tomato dried two-liter cheddar combination tomatoes sized artichoke pies every up powder yellow peppers ground pellegrinos \n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigram_generation = []\n",
    "np.random.seed(7)\n",
    "for _ in range(10):\n",
    "    s = bigramLM.generateOrder()\n",
    "    bigram_generation.append(s)\n",
    "    print(s)\n",
    "    print()\n",
    "\n",
    "pd.DataFrame({'generations': bigram_generation}).to_csv(f'{team_ID}_bigram_generations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "e8c3d102",
   "metadata": {
    "id": "e8c3d102"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two large fl ounce pizzas with a lot bit bit olives onions\n",
      "\n",
      "two 20 large ounce diet in ginger cans\n",
      "\n",
      "a sprite and a 4 ounce doctor zeroes and four 16 ml diet pellegrinos\n",
      "\n",
      "four pizzas with balsamic glaze and also pies with a much little of\n",
      "\n",
      "four pizzas with balsamic vegan and three pies with pesto\n",
      "\n",
      "two pizza with a sprite and five 16 ml red peppers\n",
      "\n",
      "i 'd like a pizza with banana peppers and salami olives\n",
      "\n",
      "i 'd like 13 high pie without dough cheese\n",
      "\n",
      "i 'd like a lunch pizza with ground cherry extra pepper bacon artichokes and dried green\n",
      "\n",
      "i 'd like a pizza with olive red tomatoes and jalapeno pepper without thin crust\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_generation = []\n",
    "np.random.seed(7)\n",
    "for _ in range(10):\n",
    "    s = rnnLM.generateOrder()\n",
    "    rnn_generation.append(s)\n",
    "    print(s)\n",
    "    print()\n",
    "\n",
    "pd.DataFrame({'generations': rnn_generation}).to_csv(f'{team_ID}_rnn_generations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e370b33",
   "metadata": {
    "id": "9e370b33"
   },
   "source": [
    "# Thank you for your efforts :D"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
